# Example configuration for aye app
#
# Looked up from XDG_CONFIG_DIR (~/.config/aye/aye.yml) on Linux or
# ~/.aye on Windows.

models:
  speechless-13b:
    path: ~/docs/models/speechless/speechless-llama2-hermes-orca-platypus-wizardlm-13b.Q5_K_S.gguf
    type: llama2
    template: alpaca
  synthia-7b:
    path: ~/docs/models/synthia-7b/synthia-7b-v1.3.Q5_K_M.gguf
    type: mistral
    template: synthia
  mistral-openorca-7b:
    path: ~/docs/models/mistral-openorca-7b/mistral-7b-openorca.Q5_K_M.gguf
    type: mistral
    template: chatml
  openhermes-mistral-7b:
    path: ~/docs/models/openhermes-2-mistral-7b/openhermes-2-mistral-7b.Q5_K_M.gguf
    type: mistral
    template: chatml
  openhermes25-mistral-7b:
    path: ~/docs/models/openhermes-25-mistral-7b/openhermes-2.5-mistral-7b.Q5_K_M.gguf
    type: mistral
    template: chatml
  magicoder-s-ds-6.7b: # doesn't work https://github.com/ggerganov/llama.cpp/issues/4419
    path: ~/docs/models/magicoder-s-ds-6.7b/magicoder-s-ds-6.7b.Q4_K_M.gguf
    type: deepseek
    template: magicoder
  deepseek-coder-6.7b:
    path: ~/docs/models/deepseek-coder-6.7b/deepseek-coder-6.7b-instruct.Q5_K_M.gguf
    type: deepseek
    template: alpaca

profiles:
  # See https://www.reddit.com/r/LocalLLaMA/comments/1343bgz/what_model_parameters_is_everyone_using/
  precise:
    # factual responses and straightforward assistant answers
    temperature: 0.7
    repeat_penalty: 1.176
    top_k: 40
    top_p: 0.1
  creative:
    # chatting, storywriting, and interesting assistant answers
    temperature: 0.72
    repeat_penalty: 1.1
    top_k: 0
    top_p: 0.73
  sphinx:
    # varied storywriting (on 30B/65B) and unconventional chatting
    temperature: 1.99
    repeat_penalty: 1.15
    top_k: 30
    top_p: 0.18

chat:
  model: openhermes25-mistral-7b
  profile: precise
  settings:
    n_threads: 11
    n_gpu_layers: 18
task:
  model: mistral-openorca-7b
  profile: precise
  settings:
    n_threads: 10
    n_gpu_layers: 20
