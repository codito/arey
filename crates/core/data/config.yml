# Example configuration for arey app
#
# Looked up from XDG_CONFIG_DIR (~/.config/arey/arey.yml) on Linux or
# C:\Users\<username>\.arey\arey.yml on Windows.
#
# Please refer to below arey configuration for more examples:
# https://github.com/codito/dotfiles/blob/master/.config/arey/arey.yml

models:
  openhermes25-mistral-7b:
    path: TODO
    type: gguf
    # uncomment to specify configuration for llama-cpp
    # n_threads: 10
    # n_gpu_layers: 20
  ollama-tinydolphin:
    name: tinydolphin:latest   # name of the model, see http://localhost:11434/api/tags
    type: ollama
  openai-example: # locally hosted openai compatible server, e.g. ollama server
    name: "gemma2-9b-it"
    type: openai
    base_url: "http://localhost:8080"
  groq: # remote openai compatible server
    name: llama-3.1-70b-versatile
    type: openai
    base_url: https://api.groq.com/openai/v1
    api_key: env:GROQ_API_KEY # defined as env variable
  # deepseek-coder-6.7b:
  #   path: ~/docs/models/deepseek-coder-6.7b/deepseek-coder-6.7b-instruct.Q5_K_M.gguf
  #   type: deepseek

profiles:
  # See https://www.reddit.com/r/LocalLLaMA/comments/1343bgz/what_model_parameters_is_everyone_using/
  precise:
    # factual responses and straightforward assistant answers
    temperature: 0.7
    repeat_penalty: 1.176
    top_k: 40
    top_p: 0.1
  creative:
    # chatting, storywriting, and interesting assistant answers
    temperature: 0.72
    repeat_penalty: 1.1
    top_k: 0
    top_p: 0.73
  sphinx:
    # varied storywriting (on 30B/65B) and unconventional chatting
    temperature: 1.99
    repeat_penalty: 1.15
    top_k: 30
    top_p: 0.18

# Colorscheme for syntax highlighting. Allowed values: light (default) or dark.
theme: light

chat:
  model: openhermes25-mistral-7b
  profile: precise
task:
  model: openhermes25-mistral-7b
  profile: precise
