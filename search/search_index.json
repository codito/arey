{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Arey","text":"<p>Arey is a command line playground app for local large language models. It supports all the models supported by Llama.cpp and Ollama.</p>"},{"location":"#install","title":"Install","text":"<p>We recommend using <code>pipx</code> to install the app in an isolated python virtual environment.</p> <pre><code># Install pipx if needed: `pip install pipx`\n# Ensure ~/.local/bin is available in system PATH\npipx install arey\n</code></pre> <p>You can upgrade to latest version of the app using <code>pipx upgrade arey</code>.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>arey chat</code> - Start a chat session with local AI model.</li> <li><code>arey ask [query]</code> - Ask the AI model to help with [query].</li> <li><code>arey play</code> - Create a playground file, edit the file and generate AI   response.</li> <li><code>arey --help</code> - Print help message and exit.</li> </ul>"},{"location":"#configure","title":"Configure","text":"<p>On the first run, <code>arey</code> will create a configuration file in following location:</p> <ul> <li><code>~/.config/arey/arey.yml</code> for Linux or Mac systems.</li> <li><code>~/.arey/arey.yml</code> for Windows.</li> </ul> <p>Please update the <code>models</code> section in the config yml to your local model path.</p>"},{"location":"#get-the-model","title":"Get the model","text":"<p>For this quick start guide, let's download the Tiny Dolphin model locally.</p> <pre><code>$ mkdir ~/models\n$ cd ~/models\n\n# If wget is not available on your platform, open the below link\n# in your browser and save it to ~/models.\n# Size of this model: ~668MB\n$ wget https://huggingface.co/s3nh/TinyDolphin-2.8-1.1b-GGUF/resolve/main/tinydolphin-2.8-1.1b.Q4_K_M.gguf\n\n# ...\n$ ls\ntinydolphin-2.8-1.1b.Q4_K_M.gguf\n</code></pre> Where to find models? <p>You can find Llama.cpp compatible models by searching for <code>gguf</code> in https://huggingface.co/models?search=gguf.</p> <p>https://huggingface.co/TheBloke has a good collection of models you can use locally.</p> <p>We recommend the Q4_K_M or Q5_K_M quantized models.</p>"},{"location":"#update-configuration","title":"Update configuration","text":"<p>Let's add an entry for this model in arey's config file.</p> <pre><code># Example configuration for arey app\n#\n# Looked up from XDG_CONFIG_DIR (~/.config/arey/arey.yml) on Linux or\n# ~/.arey on Windows.\n\nmodels:\n  tinydolphin:\n    path: ~/models/tinydolphin-2.8-1.1b.Q4_K_M.gguf\n    type: llama\n    template: chatml\n  ollama-tinydolphin:\n    name: tinydolphin:latest # name of the model, see http://localhost:11434/api/tags\n    type: ollama\n    template: chatml\n\nprofiles:\n  # See https://www.reddit.com/r/LocalLLaMA/comments/1343bgz/what_model_parameters_is_everyone_using/\n  precise:\n    # factual responses and straightforward assistant answers\n    temperature: 0.7\n    repeat_penalty: 1.176\n    top_k: 40\n    top_p: 0.1\n  creative:\n    # chatting, storywriting, and interesting assistant answers\n    temperature: 0.72\n    repeat_penalty: 1.1\n    top_k: 0\n    top_p: 0.73\n  sphinx:\n    # varied storywriting (on 30B/65B) and unconventional chatting\n    temperature: 1.99\n    repeat_penalty: 1.15\n    top_k: 30\n    top_p: 0.18\n\nchat:\n  model: tinydolphin\n  profile: precise\ntask:\n  model: tinydolphin\n  profile: precise\n</code></pre> <p>Noteworthy changes to the configuration file:</p> <ol> <li>Line 11-14: we added a new model definition with the path of the downloaded model.</li> <li>Line 38: we instruct <code>arey</code> to use <code>tinydolphin</code> model for chat.</li> <li>Line 41: <code>arey</code> will use <code>tinydolphin</code> for the queries in ask command.</li> </ol>"},{"location":"#chatting-with-the-model","title":"Chatting with the model","text":"<p>Let's run <code>arey chat</code> to start a chat session. See below for an illustration.</p> <pre><code>\u276f arey chat\nWelcome to arey chat!\nType 'q' to exit.\n\n\u2713 Model loaded. 0.13s.\n\nHow can I help you today?\n&gt; Who are you?\n\nI am an artificial intelligence model that has been programmed to simulate human behavior, emotions, and responses based on data gathered from various sources. My primary goal is to provide\nassistance in various areas of life, including communication, problem-solving, decision-making, and learning.\n\n\n\u25fc Completed. 0.49s to first token. 2.10s total. 75.58 tokens/s. 159 tokens. 64 prompt tokens.\n&gt;\n</code></pre> <p><code>chat</code> command will simulate a conversation with the configured AI model. In every turn, it will send all messages between you and the AI model as context and ask the AI model to generate a suitable response.</p>"},{"location":"#ask-anything","title":"Ask anything","text":"<p><code>arey ask</code> command provides a quick way to run any query on the terminal. Compared to the <code>chat</code> command, <code>ask</code> will not use any session or conversation history.</p> <pre><code>\u276f arey ask \"Who are you?\"\n\nWelcome to arey ask!\n\n\u2713 Model loaded. 0.13s.\n\nI am an artificial intelligence model that has been programmed to interact with humans through text-based conversations. My primary goal is to provide assistance, support, or information as\nrequested, but my responses can vary based on the context of the conversation.\n\n\n\u25fc Completed. 0.52s to first token. 1.97s total. 72.62 tokens/s. 143 tokens. 67 prompt tokens.\n</code></pre>"},{"location":"#playground","title":"Playground","text":"<p><code>arey play</code> allows you to fine-tune a prompt. It uses a simple markdown file to represent completion settings and the prompt. On every save, <code>arey</code> will try to generate a response for the prompt.</p> <p>In the below screenshot, we have two terminals. <code>arey play</code> is running the top terminal, it created a play file and continues to monitor it for any changes. In the bottom terminal, we edit the file and upon save we can see the generated response in top terminal.</p> <p></p>"},{"location":"#next-steps","title":"Next steps","text":"<p>See the detailed guides for Llama.cpp and Ollama.</p>"},{"location":"config/","title":"Configure arey","text":"<p>This article is a detailed reference for the <code>arey</code> configuration file.</p>"},{"location":"config/#location","title":"Location","text":"<p>Arey stores the configuration file in following locations:</p> <ul> <li><code>~/.config/arey/arey.yml</code> for Linux, MAC, or WSL2 in Windows.</li> <li><code>C:\\users\\&lt;username&gt;\\.arey\\arey.yml</code> for Windows.</li> </ul> <p>Configuration file is a valid YAML file.</p> <p>A default configuration file is created by <code>arey</code> on the first run. You can refer here for the latest configuration file snapshot.</p>"},{"location":"config/#sections","title":"Sections","text":""},{"location":"config/#models","title":"Models","text":"<p>Model section provides a list of local LLM models for Llama.cpp or Ollama backends.</p> <pre><code># Example configuration for arey app\n#\n# Looked up from XDG_CONFIG_DIR (~/.config/arey/arey.yml) on Linux or\n# ~/.arey on Windows.\n\nmodels:\n  tinydolphin:\n    path: ~/models/tinydolphin-2.8-1.1b.Q4_K_M.gguf\n    type: llama\n    template: chatml\n  ollama-tinydolphin:\n    name: tinydolphin:latest # name of the model, see http://localhost:11434/api/tags\n    type: ollama\n    template: chatml\n</code></pre> <p><code>models</code> is a list of following elements. The <code>key</code>, e.g., <code>tinydolphin</code> can be user specified, it is used further to reference the model setting.</p> <ul> <li><code>name</code> (only for Ollama): specify the ollama model name. Should be a valid   name in the local library. Match with value from   http://localhost:11434/api/tags.</li> <li><code>path</code> (only for Llama.cpp): specify the model file path. Supports expansion   of user directory marker <code>~</code>.</li> <li><code>type</code>: must be <code>ollama</code> for Ollama models. Any other value is considered as   <code>llama</code> model.</li> <li><code>template</code>: conversation template used by the model. We use this for the <code>arey chat</code> command to convert user and assistant messages. See the Templates   section below for details.</li> </ul>"},{"location":"config/#profiles","title":"Profiles","text":"<p>Profiles section is a collection of settings used for generating LLM response. You can define as many profiles as necessary. The <code>key</code> e.g., <code>precise</code> is used in other sections to refer to the settings.</p> <pre><code>profiles:\n  # See https://www.reddit.com/r/LocalLLaMA/comments/1343bgz/what_model_parameters_is_everyone_using/\n  precise:\n    # factual responses and straightforward assistant answers\n    temperature: 0.7\n    repeat_penalty: 1.176\n    top_k: 40\n    top_p: 0.1\n  creative:\n    # chatting, storywriting, and interesting assistant answers\n    temperature: 0.72\n    repeat_penalty: 1.1\n    top_k: 0\n    top_p: 0.73\n  sphinx:\n    # varied storywriting (on 30B/65B) and unconventional chatting\n    temperature: 1.99\n    repeat_penalty: 1.15\n    top_k: 30\n    top_p: 0.18\n</code></pre> <p>Each <code>profile</code> can specify the completion settings specific to the model type. Below are a few common settings.</p> Parameter Value Purpose repeat_penalty 1-2 Higher value discourages repetition of token stop [] Comma separated list of stop words temperature 0.0-1.0 Lower temperature implies precise response top_k 0-30 Number of tokens to consider for sampling top_p 0.0-1.0 Lower value samples from most likely tokens <p>Ollama models: see the list of all parameters in Model file API documentation.</p> <p>Llama.cpp models: see the list of all parameters in create_completion API documentation.</p>"},{"location":"config/#chat-and-task-settings","title":"Chat and Task settings","text":"<p><code>chat</code> and <code>task</code> settings specify the model and profile for the <code>chat</code> and <code>ask</code> commands respectively.</p> <pre><code>chat:\n  model: ollama-tinydolphin\n  profile: precise\ntask:\n  model: ollama-tinydolphin\n  profile: precise\n  settings:\n    host: http://localhost:11434/\n</code></pre> <p>For either section, you can specify the model settings in a <code>settings</code> member. Following model settings are supported for each model type.</p> <p>Ollama models</p> Setting key Value Remark host http://localhost:11434/ Base url for Ollama server <p>Llama.cpp models</p> Setting key Value Remark n_threads Half of CPU count Number of threads to run n_ctx 4096 Context window size n_batch 512 Batch size n_gpu_layers 0 Number of layers to offload to GPU use_mlock False Lock the model in main memory verbose False Show verbose logs"},{"location":"config/#prompt-templates","title":"Prompt templates","text":"<p>A prompt template allows to specify tokens that are replaced during the runtime.</p> <p>See https://github.com/codito/arey/blob/master/arey/data/prompts/chatml.yml for an example.</p>"},{"location":"contribute/","title":"Contribute","text":"<p>Arey is under active development. We're building this app under a public domain license to enable usage anywhere and in any shape or form.</p> <p>We gladly accept contributions from the community.</p> <p>Please create an issue to share your feedback, any feature requests or bug reports.</p> <p>Thank you \u2764\ufe0f</p>"},{"location":"contribute/#development-notes","title":"Development notes","text":"<pre><code># Install arey locally in editable mode.\n&gt; pip install -e .\n&gt; pip install -e .\\[test\\] # optional, if you wish to run tests\n\n# Install with samples dependency if you wish to run them\n&gt; pip install -e .\\[samples\\]\n</code></pre> <p>With OPENBLAS, loading time for models is much smaller and inference is about 4-5% faster. Here's how to install <code>llama-cpp-python</code> with OPENBLAS support:</p> <pre><code>&gt; pip uninstall llama-cpp-python\n&gt; CMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --verbose\n</code></pre> <p>If you've a GPU, try the following installation instead.</p> <pre><code>&gt; pip uninstall llama-cpp-python\n&gt; CMAKE_ARGS=\"-DLLAMA_CUBLAS=ON\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --verbose\n</code></pre>"},{"location":"design/","title":"Design","text":"<p>Notes on the design of this tool.</p>"},{"location":"design/#goals","title":"Goals","text":"<ul> <li>Simplify development with local large language models. Must be usable both as   a CLI app and a library.</li> <li>Task oriented.</li> <li>Must support CPU based ggml models.</li> <li>Must be extensible for following dimensions. Preferably with configuration.</li> <li>Newer models</li> <li>Prompt formats</li> <li>Must support integration with other tools with in the ecosystem.</li> <li>Opinionated. Explicitly avoid dependency or feature bloat.</li> </ul>"},{"location":"design/#non-goals","title":"Non Goals","text":"<p>We're not going to be build yet another index, or a semantic kernel like <code>langchain</code>. We focus only on the language modeling aspect; not the storage or anything else.</p> <p>While these are true at the time of writing, we will be open to reconsider these in the future.</p> <ul> <li>Training models are not supported. This is an inference library.</li> <li>GPU support is not available (due to lack of resource/testing environment).</li> </ul>"},{"location":"design/#better-tools","title":"Better tools","text":"<p>We believe below tools are awesome and may be in the same bucket as this one.</p> <ul> <li>https://github.com/simonw/llm</li> <li>https://github.com/go-skynet/LocalAI</li> <li>https://github.com/jmorganca/ollama</li> </ul>"},{"location":"design/#approach","title":"Approach","text":""},{"location":"design/#mental-model","title":"Mental model","text":"<p>We will build upon following concepts.</p> <ul> <li>Models are <code>ggml</code> binary files that can be generating text. We will use   them for inference and performing tasks.</li> <li>Prompt format is the instruction set template for getting the model to do   anything. A model's prompt format depends on its training data.</li> <li>Example: Vicuna models follow <code>### Human:</code> and <code>### Assistant:</code> dialogue     format.</li> <li>Models and Prompt formats are coupled together.</li> <li>A prompt format can be reused across multiple models.</li> <li>Modes of usage can be <code>chat</code> based where the interaction is modeled as   a dialogue between 3 participants - SYSTEM, AI and USER; either single or   multi turn. Or, the mode can be <code>instruct</code> where a model is prompted to carry   out certain task.</li> <li>Note that <code>chat</code> can be also used to carry out tasks.</li> <li>Also, we can provide additional <code>context</code> in instruction mode.</li> <li>Models can support one or both of these modes.</li> <li>TODO: more clarification on where and how to use modes.</li> <li>Tasks are the primary interaction model for this app.</li> <li>E.g., <code>arey &lt;task&gt;</code> will perform the task.</li> <li>Task is the highest level of abstraction this library will provide.</li> <li>Task will own and manage a prompt. E.g., it will understand the prompt     parameters and will fill those at runtime.</li> <li>Task will be stateless. It will not have any knowledge of stores etc. It can     temporarily cache elements (TODO: memory?), but those will be ephemeral,     will clean up with the session.</li> <li>Prompt is the actual instruction or context shared with a model to perform   a Task. Obviously, Tasks and Prompts are coupled together.</li> <li>A task can have multiple prompts.</li> <li>A task can use multiple other tasks. E.g., chaining together.</li> <li>A prompt has a template with well known parameters that can be replaced at     runtime. E.g., <code>history</code> represents all the discussion in current session.</li> <li>Every prompt will also publish its output or response format. A task will     use these specifications.</li> </ul> <p>Scope of tasks</p> <p>Simple and self-sufficient tasks will be part of this library. They must have a single objective. Think of the Unix philosophy. They can be composed with other tasks or other apps to built larger tasks.</p> <p>Tasks can use primitives like tokenize, parse, cleanup etc. This is a core aspect of a natural language based tool.</p> <p>Tasks and Prompts can be defined by the consumer app. This library will provide abstractions to define and use those.</p> <p>Constraints</p> <p>Every task will only support streaming. A consumer app can decide whether to stream, or wait until all output is available.</p>"},{"location":"design/#example-runs","title":"Example runs","text":"<p>Let's enumerate few example scenarios to validate the mental model.</p> <p>1. Summarize a text</p> <ul> <li>Input: blob of text.</li> <li>Output: a summary of the provided text.</li> <li>Task can support various tweaks like summarize in bullets, or a paragraph.   Optionally, extract keywords etc.</li> <li>E.g., <code>arey summarize --keywords &lt;text blob&gt;</code> or <code>cat essay.md | arey summarize</code></li> <li>Implementation</li> <li>Prompt can use zero shot or few shot mechanism.</li> <li>Prompt response can be JSON. We can dynamically provide instruction to     extract keyword. Or, keyword extraction can be a separate prompt.</li> </ul> <p>2. Q &amp; A on a document</p> <ul> <li>Inputs</li> <li>Blob of text</li> <li>Conversation history (optional)</li> <li>Question</li> <li>Output</li> <li>Answer</li> <li>Implementation</li> <li>Use template variables for history to make sense of words like <code>it</code> in the     question.</li> <li>Will use DATA provided in prompt for answering.</li> </ul> <p>3. Q &amp; A on a directory of files</p> <ul> <li>Inputs: directory of files, and a question</li> <li>Output: answer</li> <li>Implementation</li> <li>Implement as a separate app.</li> <li>Semantic search to find answers and then use summarize task to create an     answer.</li> </ul>"},{"location":"llama/","title":"Working with Llama models","text":"<p>This guide provides step-by-step guidance on using Llama.cpp based models in <code>arey</code>. We will download a model from Huggingface, configure <code>arey</code> to use the model and finally run a few commands.</p> <p>Ollama is an alternate way to automatically download your models and run them locally. See our Ollama guide if you prefer an automated quick start.</p>"},{"location":"llama/#concepts","title":"Concepts","text":"<p>Llama is a family of large language models (LLMs) provided by Meta. While this was the initial open weights architecture available for local use, Mistral and their fine-tunes are quite popular too.</p> <p>LLMs are represented by the number of parameters they were trained with. E.g., 7B, 13B, 33B and 70B are usual buckets. Size of the model increases with the training parameters.</p> <p>Llama.cpp provides a quantization mechanism (GGUF) to reduce the file size and allows running these models on smaller form factors including CPU-only devices. You will see quantization in the model name. E.g., <code>Q4_K_M</code> implies 4-bit quantization.</p> <p>Choosing a quantization</p> <p>Always choose the lower quantization of a higher param model. E.g., Q4_K_M of 13B is better than Q8_K_M of 7B.</p>"},{"location":"llama/#get-the-models","title":"Get the models","text":"<p>Please use Huggingface search to find the GGUF models.</p> <p>Ollama maintains a registry of their supported models.</p> <p>https://www.reddit.com/r/LocalLLaMA/ is a fantastic community to stay updated and learn more about local models.</p> Our favorite models Model Parameters Quant Purpose OpenHermes-2.5-Mistral 7B Q4_K_M General chat Deepseek-Coder-6.7B 7B Q4_K_M Coding NousHermes-2-Solar-10.7B 11B Q4_K_M General chat <p>After you locate the huggingface repository, please download the model locally. Here's an example to download the Tiny Dolphin model.</p> <pre><code>$ mkdir ~/models\n$ cd ~/models\n\n# If wget is not available on your platform, open the below link\n# in your browser and save it to ~/models.\n# Size of this model: ~668MB\n$ wget https://huggingface.co/s3nh/TinyDolphin-2.8-1.1b-GGUF/resolve/main/tinydolphin-2.8-1.1b.Q4_K_M.gguf\n\n# ...\n$ ls\ntinydolphin-2.8-1.1b.Q4_K_M.gguf\n</code></pre>"},{"location":"llama/#configure","title":"Configure","text":"<p>Let's add an entry for this model in arey's config file.</p> <pre><code># Example configuration for arey app\n#\n# Looked up from XDG_CONFIG_DIR (~/.config/arey/arey.yml) on Linux or\n# ~/.arey on Windows.\n\nmodels:\n  tinydolphin:\n    path: ~/models/tinydolphin-2.8-1.1b.Q4_K_M.gguf\n    type: llama\n    template: chatml\n  ollama-tinydolphin:\n    name: tinydolphin:latest # name of the model, see http://localhost:11434/api/tags\n    type: ollama\n    template: chatml\n\nprofiles:\n  # See https://www.reddit.com/r/LocalLLaMA/comments/1343bgz/what_model_parameters_is_everyone_using/\n  precise:\n    # factual responses and straightforward assistant answers\n    temperature: 0.7\n    repeat_penalty: 1.176\n    top_k: 40\n    top_p: 0.1\n  creative:\n    # chatting, storywriting, and interesting assistant answers\n    temperature: 0.72\n    repeat_penalty: 1.1\n    top_k: 0\n    top_p: 0.73\n  sphinx:\n    # varied storywriting (on 30B/65B) and unconventional chatting\n    temperature: 1.99\n    repeat_penalty: 1.15\n    top_k: 30\n    top_p: 0.18\n\nchat:\n  model: tinydolphin\n  profile: precise\ntask:\n  model: tinydolphin\n  profile: precise\n</code></pre> <p>Noteworthy changes to the configuration file:</p> <ol> <li>Line 7-10: we added a new model definition with the path of the downloaded model.</li> <li>Line 38: we instruct <code>arey</code> to use <code>tinydolphin</code> model for chat.</li> <li>Line 41: <code>arey</code> will use <code>tinydolphin</code> for the queries in ask command.</li> </ol>"},{"location":"llama/#usage","title":"Usage","text":"<p>You can use <code>chat</code>, <code>ask</code> or <code>play</code> commands to run this model.</p>"},{"location":"llama/#completion-settings","title":"Completion settings","text":"<p>You can use profiles to configure <code>arey</code>. A profile is a collection of settings used for tuning the AI model's response. Usually it includes following parameters:</p> Parameter Value Purpose max_tokens 512 Maximum number of tokens to generate repeat_penalty 1-2 Higher value discourages repetition of token stop [] Comma separated list of stop words temperature 0.0-1.0 Lower temperature implies precise response top_k 0-30 Number of tokens to consider for sampling top_p 0.0-1.0 Lower value samples from most likely tokens <p>See the list of all parameters in create_completion API documentation.</p>"},{"location":"llama/#chatting-with-the-model","title":"Chatting with the model","text":"<p>Let's run <code>arey chat</code> to start a chat session. See below for an illustration.</p> <pre><code>\u276f arey chat\nWelcome to arey chat!\nType 'q' to exit.\n\n\u2713 Model loaded. 0.13s.\n\nHow can I help you today?\n&gt; Who are you?\n\nI am an artificial intelligence model that has been programmed to simulate human behavior, emotions, and responses based on data gathered from various sources. My primary goal is to provide\nassistance in various areas of life, including communication, problem-solving, decision-making, and learning.\n\n\n\u25fc Completed. 0.49s to first token. 2.10s total. 75.58 tokens/s. 159 tokens. 64 prompt tokens.\n&gt;\n</code></pre> <p>See quickstart for an example of <code>ask</code> and <code>play</code> commands.</p>"},{"location":"ollama/","title":"Working with Ollama models","text":"<p>This guide provides step-by-step guidance on using Ollama based models in <code>arey</code>. We will download a model with <code>ollama</code>, configure <code>arey</code> to use the model and finally run a few commands.</p> <p>Llama.cpp is an alternate way to download your models and run them locally. See our Llama guide if you prefer more customizable way to manage your models.</p>"},{"location":"ollama/#concepts","title":"Concepts","text":"<p>Llama is a family of large language models (LLMs) provided by Meta. While this was the initial open weights architecture available for local use, Mistral and their fine-tunes are quite popular too.</p> <p>LLMs are represented by the number of parameters they were trained with. E.g., 7B, 13B, 33B and 70B are usual buckets. Size of the model increases with the training parameters.</p> <p>Llama.cpp provides a quantization mechanism (GGUF) to reduce the file size and allows running these models on smaller form factors including CPU-only devices. You will see quantization in the model name. E.g., <code>Q4_K_M</code> implies 4-bit quantization.</p> <p>Choosing a quantization</p> <p>Always choose the lower quantization of a higher param model. E.g., Q4_K_M of 13B is better than Q8_K_M of 7B.</p>"},{"location":"ollama/#get-the-models","title":"Get the models","text":"<p>Ollama maintains a registry of their supported models.</p> <p>https://www.reddit.com/r/LocalLLaMA/ is a fantastic community to stay updated and learn more about local models.</p> Our favorite models Model Parameters Quant Purpose OpenHermes-2.5-Mistral 7B Q4_K_M General chat Deepseek-Coder-6.7B 7B Q4_K_M Coding NousHermes-2-Solar-10.7B 11B Q4_K_M General chat <p>After you locate the ollama model info, please run it in ollama. In this example, we will try the Tiny Dolphin model.</p> <pre><code>ollama run tinydolphin\n</code></pre>"},{"location":"ollama/#configure","title":"Configure","text":"<p>Let's add an entry for this model in arey's config file.</p> <pre><code># Example configuration for arey app\n#\n# Looked up from XDG_CONFIG_DIR (~/.config/arey/arey.yml) on Linux or\n# ~/.arey on Windows.\n\nmodels:\n  tinydolphin:\n    path: ~/models/tinydolphin-2.8-1.1b.Q4_K_M.gguf\n    type: llama\n    template: chatml\n  ollama-tinydolphin:\n    name: tinydolphin:latest # name of the model, see http://localhost:11434/api/tags\n    type: ollama\n    template: chatml\n\nprofiles:\n  # See https://www.reddit.com/r/LocalLLaMA/comments/1343bgz/what_model_parameters_is_everyone_using/\n  precise:\n    # factual responses and straightforward assistant answers\n    temperature: 0.7\n    repeat_penalty: 1.176\n    top_k: 40\n    top_p: 0.1\n  creative:\n    # chatting, storywriting, and interesting assistant answers\n    temperature: 0.72\n    repeat_penalty: 1.1\n    top_k: 0\n    top_p: 0.73\n  sphinx:\n    # varied storywriting (on 30B/65B) and unconventional chatting\n    temperature: 1.99\n    repeat_penalty: 1.15\n    top_k: 30\n    top_p: 0.18\n\nchat:\n  model: ollama-tinydolphin\n  profile: precise\ntask:\n  model: ollama-tinydolphin\n  profile: precise\n  settings:\n    host: http://localhost:11434/\n</code></pre> <p>Noteworthy changes to the configuration file:</p> <ol> <li>Line 11-14: we added a new model definition with the path of the downloaded model.</li> <li>Line 38: we instruct <code>arey</code> to use <code>tinydolphin</code> model for chat.</li> <li>Line 41: <code>arey</code> will use <code>tinydolphin</code> for the queries in ask command.</li> </ol>"},{"location":"ollama/#usage","title":"Usage","text":"<p>You can use <code>chat</code>, <code>ask</code> or <code>play</code> commands to run this model.</p>"},{"location":"ollama/#completion-settings","title":"Completion settings","text":"<p>You can use profiles to configure <code>arey</code>. A profile is a collection of settings used for tuning the AI model's response. Usually it includes following parameters:</p> Parameter Value Purpose num_predict 512 Maximum number of tokens to generate repeat_penalty 1-2 Higher value discourages repetition of token stop [] Comma separated list of stop words temperature 0.0-1.0 Lower temperature implies precise response top_k 0-30 Number of tokens to consider for sampling top_p 0.0-1.0 Lower value samples from most likely tokens <p>See the list of all parameters in Model file API documentation.</p>"},{"location":"ollama/#chatting-with-the-model","title":"Chatting with the model","text":"<p>Let's run <code>arey chat</code> to start a chat session. See below for an illustration.</p> <pre><code>\u276f arey chat\nWelcome to arey chat!\nType 'q' to exit.\n\n\u2713 Model loaded. 0.13s.\n\nHow can I help you today?\n&gt; Who are you?\n\nI am an artificial intelligence model that has been programmed to simulate human behavior, emotions, and responses based on data gathered from various sources. My primary goal is to provide\nassistance in various areas of life, including communication, problem-solving, decision-making, and learning.\n\n\n\u25fc Completed. 0.49s to first token. 2.10s total. 75.58 tokens/s. 159 tokens. 64 prompt tokens.\n&gt;\n</code></pre> <p>See quickstart for an example of <code>ask</code> and <code>play</code> commands.</p>"},{"location":"windows/","title":"Windows usage","text":"<p>We recommend using <code>arey</code> in wsl2 for the best possible experience.</p> <p>A brief troubleshooting guide for errors while using <code>arey</code> on windows.</p>"},{"location":"windows/#llama-cpp-python-installation","title":"llama-cpp-python installation","text":"<p>Symptom</p> <pre><code>\u276f pipx install arey\nFatal error from pip prevented installation. Full pip output in file:\n    C:\\Users\\codito\\AppData\\Local\\pipx\\pipx\\Logs\\cmd_2024-01-22_12.27.53_pip_errors.log\n\npip failed to build package:\n    llama-cpp-python\n\nSome possibly relevant errors from pip install:\n    error: subprocess-exited-with-error\n    no such file or directory\n    CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\n    CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage\n\nError installing arey.\n</code></pre> <p>See https://github.com/abetlen/llama-cpp-python#windows-notes.</p>"},{"location":"windows/#im-using-wsl2-but-the-model-load-is-extremely-slow","title":"I'm using WSL2 but the model load is extremely slow","text":"<p>Ensure that the <code>*.gguf</code> file is loaded from a native ext4 partition.</p> <pre><code># You downloaded models can be in C:\\Users\\&lt;user&gt;\\Downloads\\ directory\n# Copy those to native WSL2 file system\n\n&gt; mkdir ~/models\n&gt; cp /mnt/c/Users/codito/Downloads/openhermes-2.5-mistral-7b.Q5_K_M.gguf ~/models/\n\n# Now use ~/models/openhermes-2.5-mistral-7b.Q5_K_M.gguf in ~/.config/arey/arey.yml\n</code></pre>"},{"location":"windows/#slow-performance-due-to-less-memory-and-cpu","title":"Slow performance due to less memory and cpu","text":"<p>Create a <code>C:\\Users\\&lt;user&gt;\\.wslconfig</code> with appropriate CPU and Memory configuration. See https://learn.microsoft.com/en-us/windows/wsl/wsl-config#wslconfig.</p> <p>Before creating below file, ensure WSL is shutdown with <code>wsl --shutdown</code>.</p> <pre><code># Contents of .wslconfig\nmemory=32GB      # for a machine with 64GB ram, change based on your system\nprocessors=16    # for a machine with 16 logical processors\nswap=4GB\n</code></pre> <p>Save this file and start wsl.</p>"}]}